# ğŸ§  Recursive Medical Gemma Agent (R-MedLM)

A **recursive, tool-using clinical reasoning system** built around **Gemma LLMs** and structured medical data.

This project implements a **Recursive Language Model (RLM) architecture** for medical analysis, where the model:
- Writes executable Python code
- Uses structured medical tools
- Recursively calls itself for focused reasoning
- Operates inside a persistent REPL environment

The system is designed for **research in long-horizon medical reasoning** and **agentic LLM workflows**.

---

## ğŸ—ï¸ Architecture Overview

User Query
â†“
Root Model (Gemma via Ollama)
â†“
Writes Python code
â†“
REPL Environment
â†“
Medical Tool Calls (labs, vitals, cohorts)
â†“
Optional Recursive Sub-calls
â†“
Final structured result

---

## ğŸš€ Features

âœ” Recursive LLM reasoning  
âœ” Persistent execution environment (REPL)  
âœ” Modular medical tool layer  
âœ” Cohort-based analysis  
âœ” Local LLM inference using **Ollama + Gemma**  
âœ” No cloud dependency required  

---

## ğŸ’» System Requirements

- macOS (Apple Silicon recommended) or Linux
- Python **3.10+**
- Ollama installed locally
- At least **8GB RAM** recommended

---

## ğŸ§  Model Setup (Gemma)

This project runs **Gemma locally** using Ollama.

### 1ï¸âƒ£ Install Ollama

```bash
brew install ollama
```

### Start the Ollama server:
```bash
ollama serve
```
### 2ï¸âƒ£ Pull the Gemma Model
```bash
ollama pull gemma:2b-instruct
```
## ğŸ“¦ Installation
Clone the repo and install dependencies:
```bash
git clone <your-repo-url>
cd medgemma

python -m venv venv
source venv/bin/activate

pip install -r requirements.txt
```
## â–¶ï¸ Running the System

Run the recursive medical agent:
```bash
python run_pipeline.py
```
You should see iterative reasoning steps like:
```bash
ğŸ” Iteration 1
ğŸ§  Model generated code:
patients = cohort_tools.get_patients_with_diagnosis("A41")
...
Final = results
```

## ğŸ—„ï¸ Data Layer

The system currently uses DuckDB for local structured storage.

Future extensions may include:
	â€¢	MIMIC-IV integration
	â€¢	EHR pipelines
	â€¢	Real-time streaming data

## ğŸ§© Project Structure

```code
medgemma/
â”‚
â”œâ”€â”€ rlm_core/        # Recursive controller + REPL
â”œâ”€â”€ models/          # LLM interfaces (Ollama-based)
â”œâ”€â”€ tools/           # Medical data tool functions
â”œâ”€â”€ mimic_memory/    # Database and schema layer
â”œâ”€â”€ prompts/         # System prompts for root & subcall models
â”œâ”€â”€ config/          # Model and DB configs
â””â”€â”€ run_pipeline.py  # Main entry point
```
## ğŸ§ª Development Notes
This project is an experimental research system exploring:
	â€¢	Recursive LLM inference
	â€¢	Symbolic + neural hybrid reasoning
	â€¢	Tool-based clinical analysis

Expect frequent updates and evolving APIs.
```
